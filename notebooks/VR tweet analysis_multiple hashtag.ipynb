{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of excel files downloaded from Tweet Archivist\n",
    "filenames = [\"all tweet data_thru July.xlsx\",\"VR_export_200707_200727.xlsx\"]\n",
    "month = 7\n",
    "#Hashtag(s) in lower case\n",
    "excludedHTs = [\"protectourvote\",\"hr1\",\"voteforourlives\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_files(filenames):\n",
    "    '''(list of strings -> df) Takes list of filenames, imports each one into pandas,\n",
    "    then concatenates them into a single df\n",
    "    '''\n",
    "    \n",
    "    #import dfs and put in a list\n",
    "    df_list = []\n",
    "    for filename in filenames:\n",
    "        filedf = pd.read_excel(filename)\n",
    "        df_list.append(filedf)\n",
    "    \n",
    "    #concat list if contains more than 1 df\n",
    "    if len(df_list) > 1:\n",
    "        df = pd.concat(df_list)\n",
    "    else:\n",
    "        df = df_list[0]\n",
    "            \n",
    "    return df\n",
    "\n",
    "\n",
    "def month_filter(df, month):\n",
    "    '''(df, integer > df) Takes the Tweet Archvisit df and a month, and filters the df to only include \n",
    "    Tweets in that month'''\n",
    "\n",
    "    #Convert date column from string to datetime object\n",
    "    df[\"Local Time Stamp\"] = pd.to_datetime(df[\"Local Time Stamp\"])\n",
    "\n",
    "    #Extract month and year from the datetime column and map to new columns\n",
    "    df[\"Month\"] = df[\"Local Time Stamp\"].map(lambda x: x.month)\n",
    "    df[\"Year\"] = df[\"Local Time Stamp\"].map(lambda x: x.year)\n",
    "\n",
    "    #filter for only the current month\n",
    "    df = df[df['Month'] == month]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_hashtags(df, excludedHT):\n",
    "    '''(df, string > df) Takes Tweet Archivist df, splits hashtag column, and returns a new df with each hashtag as a row, \n",
    "    excluding the hashtag being analyzed'''\n",
    "\n",
    "    #Split hashtag column, drop nas and put in list\n",
    "    split_htlist = df.Hashtags.str.split(\" \").dropna().to_list()\n",
    "\n",
    "    #Flatten list of lists\n",
    "    htlist = [item for sublist in split_htlist for item in sublist]\n",
    "\n",
    "    #Remove Protectourvote\n",
    "    hashtags = pd.DataFrame(list(filter(lambda a: a != excludedHT, htlist)))\n",
    "    \n",
    "    return hashtags\n",
    "\n",
    "\n",
    "def get_words(df):\n",
    "    '''(df > df) Takes Tweet Archivist df, splits Tweet text column, removes hashtags, usernames\n",
    "    non-alpha strings and stopwords'''\n",
    "    \n",
    "    #Make tweet text lower case\n",
    "    df.Text = df.Text.str.lower()\n",
    "\n",
    "    #Split text column, drop nas and put in list\n",
    "    split_wordlist = df.Text.str.split(\" \").dropna().to_list()\n",
    "\n",
    "    #Flatten list of lists\n",
    "    wordlist = [item for sublist in split_wordlist for item in sublist]\n",
    "\n",
    "    #htlistwithhash = [\"#\" + word for word in htlist]\n",
    "    \n",
    "    #remove hashtags\n",
    "    hashlist = [word for word in wordlist if word.startswith(\"#\")]\n",
    "    wordlist =  [word for word in wordlist if word not in hashlist]\n",
    "    #remove usernames\n",
    "    userlist = [word for word in wordlist if word.startswith(\"@\")]\n",
    "    wordlist = [word for word in wordlist if word not in userlist]\n",
    "\n",
    "    #make wordlist into single string\n",
    "    words = \",\".join(wordlist)\n",
    "\n",
    "    #Use NLTK tokenize to split string\n",
    "    words = word_tokenize(words) \n",
    "\n",
    "    #Remove non-alpha words\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "\n",
    "    #Set stopwords and remove\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "\n",
    "    #create new df of all words\n",
    "    words = pd.DataFrame(list(filter(lambda a: a != \"rt\", words)), columns = [\"Words\"])\n",
    "\n",
    "              \n",
    "    #Remove stray unneeded words\n",
    "    words = words[words.Words != 'amp']\n",
    "    words = words[words.Words != 'https']\n",
    "              \n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauranixon/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py:5208: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protectourvote\n",
      "Total tweets\n",
      "4161\n",
      "Unique users\n",
      "3078\n",
      "Potential Impressions\n",
      "34830976\n",
      "\n",
      "\n",
      "hr1\n",
      "Total tweets\n",
      "621\n",
      "Unique users\n",
      "544\n",
      "Potential Impressions\n",
      "2672173\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauranixon/opt/anaconda3/lib/python3.7/site-packages/xlsxwriter/worksheet.py:931: UserWarning: Ignoring URL 'https://t.co/tga0nt9uvzyoung%20badgers%20are%20needed%20to%20be%20poll%20workers,%20as%20the%20%22older%20folks%22%20who%20typically%20do%20it,%20aren't%20able%20to,%20due%20to%20being%20high%20risk%20for%20covid-19.time%20is%20of%20the%20essence!%20%20apply%20now!%20%20' with link or location/anchor > 255 characters since it exceeds Excel's limit for URLS\n",
      "  force_unicode(url))\n",
      "/Users/lauranixon/opt/anaconda3/lib/python3.7/site-packages/xlsxwriter/worksheet.py:931: UserWarning: Ignoring URL 'https://t.co/mjdqyxqpur%20please%20watch%20this%20on%20' with link or location/anchor > 255 characters since it exceeds Excel's limit for URLS\n",
      "  force_unicode(url))\n",
      "/Users/lauranixon/opt/anaconda3/lib/python3.7/site-packages/xlsxwriter/worksheet.py:931: UserWarning: Ignoring URL 'https://t.co/klobhafjkigotta%20love%20gop.%20people%20are%20being%20kicked%20out%20of%20their%20homes%20and%20more%20and%20more%20people%20die%20from%20this%20every%20day%20and%20they%20are%20worried%20about%20spending%20some%20money%20to%20help%20those%20people.%20' with link or location/anchor > 255 characters since it exceeds Excel's limit for URLS\n",
      "  force_unicode(url))\n",
      "/Users/lauranixon/opt/anaconda3/lib/python3.7/site-packages/xlsxwriter/worksheet.py:931: UserWarning: Ignoring URL 'https://www.eleven-films.com/_api/media-share-server-for-video/btgdq?instance-id=5ec8a3b3-1ac0-4556-9178-c4eb775e7cd9&component-id=comp-k7cqjuac2&channel-id=134bf377e2404f408e8045b914b259e8&video-id=dad25cd5874b45e1ac9e3c0f32df1b36&bi-token=f691e669-cc19-0fe2-1e99-d7a58e18a455' with link or location/anchor > 255 characters since it exceeds Excel's limit for URLS\n",
      "  force_unicode(url))\n",
      "/Users/lauranixon/opt/anaconda3/lib/python3.7/site-packages/xlsxwriter/worksheet.py:931: UserWarning: Ignoring URL 'https://secure.actblue.com/donate/bvf_gs_ads_dd?refcode=omvf_gs_bvf_d2d_donate&gclid=EAIaIQobChMIjJmMu-TS6gIVVh6tBh2WDAfGEAAYASAAEgL5xvD_BwE%20https://www.independent.co.uk/news/world/americas/us-election/donald-trump-chance-of-winning-election-2020-joe-biden-poll-model-a9609236.html' with link or location/anchor > 255 characters since it exceeds Excel's limit for URLS\n",
      "  force_unicode(url))\n",
      "/Users/lauranixon/opt/anaconda3/lib/python3.7/site-packages/xlsxwriter/worksheet.py:931: UserWarning: Ignoring URL 'https://www.npr.org/2020/07/17/892293736/justice-ruth-bader-ginsburg-has-cancer-again-says-she-will-remain-on-the-court?utm_term=nprnews&utm_campaign=npr&utm_medium=social&utm_source=facebook.com&fbclid=IwAR1FvwzAbEHa_lTmI75Q6q75GqfYlu6vliUQFOEUdZ6pZn8GOM7iZ0RYkdw' with link or location/anchor > 255 characters since it exceeds Excel's limit for URLS\n",
      "  force_unicode(url))\n",
      "/Users/lauranixon/opt/anaconda3/lib/python3.7/site-packages/xlsxwriter/worksheet.py:931: UserWarning: Ignoring URL 'https://m.tiktok.com/v/6851632556589321478.html?_d=secCgsIARCbDRgBIAIoARI%2BCjyqv6%2Bal9MmR9cZZ3HFOgMK9YiGo%2Bp8D1%2FQv2LYFr6D%2FLZXu2v2c1x%2F%2F3%2B4%2BgPuGeMbg8pAPQSTlGXI3rUaAA%3D%3D&language=en&preview_pb=0&share_item_id=6851632556589321478&timestamp=1595546439&tt_from=copy&u_code=2f4kbm&user_id=17129824&utm_campaign=client_share&utm_medium=ios&utm_source=copy' with link or location/anchor > 255 characters since it exceeds Excel's limit for URLS\n",
      "  force_unicode(url))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voteforourlives\n",
      "Total tweets\n",
      "35377\n",
      "Unique users\n",
      "24627\n",
      "Potential Impressions\n",
      "124185778\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import and concatenate file(s)\n",
    "df = import_files(filenames)\n",
    "\n",
    "#filter for current month\n",
    "df = month_filter(df, month)\n",
    "\n",
    "#Use regex to create extract hashtags to Hashtags column\n",
    "#Tweet Archivists' Hashtags column seems to leave some out\n",
    "df[\"Hashtags\"] = df[\"Text\"].str.findall(r'#(\\w+)')\n",
    "#join the list of strings returned by findall, and make lower case\n",
    "df['Hashtags'] = df[\"Hashtags\"].apply(lambda x: ' '.join([str(i) for i in x])).str.lower()\n",
    "\n",
    "\n",
    "#For hashtags in the list, filter for only tweets with that hashtag, \n",
    "#then get hashtags, words & tweet counts\n",
    "for excludedHT in excludedHTs:\n",
    "\n",
    "    #filter for only DFs with that particular hashtag\n",
    "    htdf = df[df[\"Hashtags\"].fillna(\"\").str.contains(excludedHT)]\n",
    "    \n",
    "    #get tuple of hashtags df and hashtags list\n",
    "    hashtags = get_hashtags(htdf, excludedHT)\n",
    "\n",
    "    #get df of words\n",
    "    words = get_words(htdf)\n",
    "    \n",
    "    #export df, word and hashtag lists to excel\n",
    "    htdf.to_excel(excludedHT+\" data.xlsx\", sheet_name=\"#protecto\",index=False)\n",
    "    words.to_excel(excludedHT+\" Tweet words.xlsx\", index=False)\n",
    "    hashtags.to_excel(excludedHT+\" Hashtags.xlsx\", index=False)\n",
    "\n",
    "    print(excludedHT)\n",
    "    print(\"Total tweets\")\n",
    "    print(len(htdf))\n",
    "    print(\"Unique users\")\n",
    "    print(len(set(htdf[\"User Name\"])))\n",
    "    print(\"Potential Impressions\")\n",
    "    print(sum(htdf[\"Follower Count\"]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
